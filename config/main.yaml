defaults:
  - _self_

# Experiment naming and logging
exp_name: default
timestamp: ${now:%Y-%m-%d_%H-%M-%S}
run_name: ${exp_name}_${timestamp}

# Weights & Biases settings
wandb:
  project: "rsp-training"
  entity: happyhappy  # Set to your wandb username or team name
  mode: "online"  # Set to "disabled" to turn off wandb logging

# Dataset and training loop configurations are moved to specific configs

# Training parameters
batch_size: 24
epochs: 400
accum_iter: 16
device: "cuda"
seed: 0
resume: false
start_epoch: 0
output_dir: outputs/${run_name}
log_dir: logs/${run_name}

# Model parameters
model: "rsp_vit_small_patch16"
input_size: 224
context_emb_dim: 1536
norm_pix_loss: true
dtype: "float32"  # Options: float32, float16, bfloat16
mask_ratio: 0.75
noise_scale: 0.5
kl_scale: 0.01
kl_balance: 0.2
kl_freebit: 0.1
stoch: 32
discrete: 32

# Optimizer parameters
weight_decay: 0.05
lr: null
blr: 1.5e-4
min_lr: 0.0
warmup_epochs: 40

# Dataset parameters
max_distance: 48
repeated_sampling: 2
num_workers: 6
pin_mem: true
prefetch_factor: 2

# Distributed training parameters
distributed: true
dist_backend: "nccl"
world_size: 4
gpu: -1
rank: 0
local_rank: -1
dist_on_itp: false
dist_url: "env://"
