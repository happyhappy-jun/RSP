defaults:
  - _self_
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

# Experiment naming and logging
exp_name: default
timestamp: ${now:%Y-%m-%d_%H-%M-%S}
run_name: ${exp_name}_${timestamp}

# Weights & Biases settings
wandb:
  project: "rsp-training"
  entity: happyhappy  # Set to your wandb username or team name
  mode: "online"  # Set to "disabled" to turn off wandb logging
  resume: allow

dataset: ???

train_loop:
  _target_: engine_pretrain_repsamp.train_one_epoch_llm
  _partial_: true

# Training parameters
batch_size: 24
epochs: 400
accum_iter: 16
device: "cuda"
seed: 0
resume: false
start_epoch: 0
output_dir: outputs/${run_name}
log_dir: logs/${run_name}

# Model parameters
model_name: "rsp_vit_small_patch16"
amp: true
input_size: 224
context_emb_dim: 1536
dtype: "float32"  # Options: float32, float16, bfloat16

model_params:
  norm_pix_loss: true
  kl_scale: 0.001
  kl_balance: 0.2
  kl_freebit: 0.1
  stoch: 32
  discrete: 32
  mask_ratio: 0.75
  noise_scale: 0.5

# Optimizer parameters
weight_decay: 0.05
lr: null
blr: 1.5e-4
min_lr: 0.0
warmup_epochs: 40

# Dataset parameters
max_distance: 48
repeated_sampling: 2
num_workers: 6
pin_mem: true
prefetch_factor: 2

# Distributed training parameters
distributed: true
dist_backend: "nccl"
world_size: 4
gpu: -1
rank: 0
local_rank: -1
dist_on_itp: false
dist_url: "env://"
