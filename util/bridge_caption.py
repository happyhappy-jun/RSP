import os
import random
import logging
import pickle
import io
import glob
import numpy as np
import torch
from PIL import Image
from torch.utils.data import Dataset, DataLoader, IterableDataset
from torchvision import transforms
from typing import Any, Dict, List, Optional
from tqdm import tqdm
import webdataset as wds
from transformers import AutoTokenizer
from util.transform import PairedRandomResizedCrop

logging.basicConfig(
    level=logging.INFO,
    handlers=[logging.StreamHandler(), logging.FileHandler("traj_reader_iter.log")],
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
)
logger = logging.getLogger(__name__)

def bytes_to_numpy(byte_data):
    """
    Convert raw bytes (JPEG/PNG) into a NumPy array (H, W, C).
    """
    with io.BytesIO(byte_data) as buffer:
        with Image.open(buffer) as img:
            img = img.convert("RGB")
            arr = np.array(img)
    return arr

class BridgeCaptionIterableDataset(IterableDataset):
    """
    Iterable-style Dataset for reading shards generated by WebDataset with proper distributed training support.
    
    This uses WebDataset's built-in mechanisms for sharding data across multiple workers/nodes.
    """

    def __init__(
        self,
        wds_pattern: str,
        repeated_sampling: int = 2,
        interval: int = 4,
        tokenizer_name: str = "Alibaba-NLP/gte-large-en-v1.5",
        max_length: int = 128,
        seed: int = 42,
        world_size: int = 1,
        rank: int = 0,
        shuffle_shards: int = 1000,
    ):
        super().__init__()
        self.wds_pattern = wds_pattern
        self.shard_files = sorted(glob.glob(wds_pattern))
        if not self.shard_files:
            raise FileNotFoundError(f"No files match pattern {wds_pattern}")

        self.repeated_sampling = repeated_sampling
        self.interval = interval
        self.max_length = max_length
        self.world_size = world_size
        self.rank = rank
        self.shuffle_shards = shuffle_shards
        self.seed = seed
        
        logger.info(f"Iterable Dataset: found {len(self.shard_files)} shards using pattern {wds_pattern}")
        
        # Pairwise transform
        self.pair_transform = PairedRandomResizedCrop(seed=seed, hflip_p=0)

        # Basic PIL -> Tensor transform + normalization
        self.basic_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225]),
        ])

        # HF tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    def tokenize_text(self, text: str) -> Dict[str, torch.Tensor]:
        """
        Tokenize a single string. Returns a dict of Tensors, each shape (seq_len,).
        """
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {k: v.squeeze(0) for k, v in encoding.items()}

    def process_sample(self, sample):
        """Process a single WebDataset sample into (src_image, tgt_image, captions)"""
        try:
            # Extract pickled data
            images_pkl = sample["images.pkl"]
            moves_pkl = sample["moves.pkl"]

            images_list = pickle.loads(images_pkl)  # list of raw bytes
            moves_list = pickle.loads(moves_pkl)

            length = len(images_list)
            if length == 0:
                return None
            
            # Generate random pairs based on repeated_sampling
            for _ in range(self.repeated_sampling):
                src_idx = random.randint(0, length - 1)
                tgt_idx = min(src_idx + self.interval, length - 1)
                
                try:
                    # Convert raw bytes -> NumPy array (H, W, 3)
                    src_arr = bytes_to_numpy(images_list[src_idx])
                    tgt_arr = bytes_to_numpy(images_list[tgt_idx])

                    # Apply pairwise transform => returns PIL images
                    src_cropped, tgt_cropped = self.pair_transform(src_arr, tgt_arr)

                    # Convert PIL -> Tensor, normalized
                    src_tensor = self.basic_transform(src_cropped)
                    tgt_tensor = self.basic_transform(tgt_cropped)

                    # Tokenize text
                    caption_text = moves_list[src_idx]
                    token_dict = self.tokenize_text(caption_text)

                    return {
                        "src_images": src_tensor,
                        "tgt_images": tgt_tensor,
                        "captions": token_dict
                    }
                except Exception as e:
                    logger.warning(f"Error processing images: {e}")
                    continue
            
            return None
        except Exception as e:
            logger.warning(f"Error processing sample: {e}")
            return None

    def __iter__(self):
        """
        Create a WebDataset pipeline with proper multi-node/worker splitting
        and return an iterator of processed samples.
        """
        worker_info = torch.utils.data.get_worker_info()
        
        # Set different seed for each rank and worker
        worker_seed = self.seed
        if worker_info is not None:
            worker_seed = worker_seed + worker_info.id
        if self.rank is not None:
            worker_seed = worker_seed + 10000 * self.rank
        
        random.seed(worker_seed)
        torch.manual_seed(worker_seed)
        
        # Create a WebDataset pipeline with proper node and worker splitting
        dataset = wds.WebDataset(
            self.shard_files, 
            shardshuffle=self.shuffle_shards if self.shuffle_shards > 0 else False,
            nodesplitter=wds.shardlists.split_by_node,  # Crucial for multi-node training
            handler=wds.warn_and_continue
        )
        
        # Enable node splitting (for multi-node DDP)
        if self.world_size > 1:
            dataset = dataset.split_by_node(self.world_size, self.rank)
        
        # Enable worker splitting (for multiple workers per GPU)
        if worker_info is not None:
            dataset = dataset.split_by_worker(worker_info.num_workers, worker_info.id)
        
        # Process each sample
        for sample in dataset:
            processed = self.process_sample(sample)
            if processed is not None:
                yield processed


def create_data_loader(
    wds_pattern, 
    batch_size=32, 
    num_workers=4, 
    repeated_sampling=2, 
    interval=4, 
    shuffle_shards=1000,
    world_size=1,
    rank=0,
    seed=42,
    pin_memory=True,
    prefetch_factor=2,
):
    """Helper function to create a data loader with the right dataset configuration."""
    dataset = BridgeCaptionIterableDataset(
        wds_pattern=wds_pattern,
        repeated_sampling=repeated_sampling,
        interval=interval,
        world_size=world_size,
        rank=rank,
        shuffle_shards=shuffle_shards,
        seed=seed,
    )
    
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=pin_memory,
        prefetch_factor=prefetch_factor,
        # No need for a DistributedSampler with IterableDataset
    )
    
    return loader


if __name__ == "__main__":
    import torch.distributed as dist
    
    # Initialize distributed environment if needed
    try:
        dist.init_process_group(backend="nccl")
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        device = torch.device(f"cuda:{local_rank}")
        torch.cuda.set_device(device)
        is_distributed = True
        print(f"Initialized process group: rank={rank}, world_size={world_size}, local_rank={local_rank}")
    except Exception as e:
        print(f"Not using distributed mode: {e}")
        rank = 0
        world_size = 1
        is_distributed = False
    
    # Example usage
    wds_url = "/root/RSP/demo/webdataset_shards_repeated/bridge_arm_move-*.tar.gz"
    loader = create_data_loader(
        wds_pattern=wds_url,
        batch_size=4,
        num_workers=2,
        world_size=world_size,
        rank=rank,
        shuffle_shards=1000,
        seed=42,
    )
    
    # Count total samples to verify
    sample_count = 0
    for i, batch in enumerate(loader):
        print(
            f"Rank {rank}: Batch {i}: src_images={batch['src_images'].shape}, "
            f"tgt_images={batch['tgt_images'].shape}, "
            f"captions keys={list(batch['captions'].keys())}"
        )
        sample_count += batch['src_images'].shape[0]
        if i >= 5:
            break
    
    print(f"Rank {rank}: Loaded {sample_count} samples successfully")
    
    # Clean up
    if is_distributed:
        dist.destroy_process_group()