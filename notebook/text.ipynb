{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junyoon/.local/share/virtualenvs/RSP-HgGdQf5B/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e720468c234bc78997f00d6db40cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "# If you have an 80G A100 GPU, you can put the entire model on a single GPU.\n",
    "# Otherwise, you need to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
    "path = 'OpenGVLab/InternVL2_5-8B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "image_path1 = \"/data/RSP/rlbench_frames/insert_onto_square_peg_variation0_episode0_front/pair_2_frame0.jpg\"  # Replace with your image path\n",
    "image_path2 = \"/data/RSP/rlbench_frames/insert_onto_square_peg_variation0_episode0_front/pair_2_frame1.jpg\"  # Replace with your image path\n",
    "\n",
    "\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "# multi-image multi-round conversation, combined images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÊãºÊé•ÂõæÂÉè)\n",
    "pixel_values1 = load_image(image_path1, max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image(image_path2, max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "prompt = \"\"\"You are a movement analyzer specialized in comparing consecutive video frames. Analyze and describe changes between frames using the following guidelines:\n",
    "FORMAT:\n",
    "- Provide one clear sentence summarizing the overall dynamics and activity\n",
    "- Describe qualitative and relative dynamics between frames\n",
    "- Use precise directional terms (left, right, up, down, forward, backward)\n",
    "- Focus on observable, concrete changes\n",
    "\n",
    "ANALYZE THE FOLLOWING ELEMENTS:\n",
    "- Main Subject/Object:\n",
    "    - Position: Track center of mass movement\n",
    "    - Rotation: Note any turns or spins\n",
    "    - Orientation: Describe facing direction\n",
    "    - State Changes: Document visible changes in:\n",
    "        - Physical form or shape\n",
    "        - Color or appearance\n",
    "        - Expression or emotional state (if applicable)\n",
    "- Background:\n",
    "    - Note any changes in background elements\n",
    "    - Identify moving vs static elements\n",
    "\n",
    "Keep descriptions concise, objective, and focused on visible changes between frames.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f47420a59fe9881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Describe the changes between the two frames.\n",
      "\n",
      "Assistant: The two frames show a robot arm in a 3D simulation environment. Here are the changes between the two frames:\n",
      "\n",
      "1. **Robot Arm Position**: The robot arm has moved slightly. In the first frame, the arm is positioned more towards the left, while in the second frame, it is slightly more towards the right.\n",
      "\n",
      "2. **End Effector Position**: The end effector (the part of the robot arm that interacts with objects) has moved. In the first frame, the end effector is closer to the base of the robot arm, while in the second frame, it is slightly further away from the base.\n",
      "\n",
      "3. **Object Position**: The objects on the table have moved. In the first frame, the objects are positioned more towards the left side of the table, while in the second frame, they are slightly more towards the right side of the table.\n",
      "\n",
      "4. **Lighting and Shadows**: The lighting and shadows in the environment have changed slightly. The shadows of the robot arm and the objects on the table are more pronounced in the second frame compared to the first frame.\n",
      "\n",
      "These changes indicate that the robot arm is performing some kind of movement or action, possibly interacting with the objects on the table.\n"
     ]
    }
   ],
   "source": [
    "question = ('<image>\\n'\n",
    "            'Describe the changes between the two frames.\\n'\n",
    "            )\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c1a96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5cff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb19eda148d406f891710d299c5a07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 32 files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3931f1e6b4432fbdea1c327ce65402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50458c85b6e84127a1a9e43305f7d3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_internlm2_fast.py:   0%|          | 0.00/7.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6eb91f3854479bacf1f1310156881c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43454ff05d494211bb85c93f9b62389a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0dfbf8297a4217b4f82d4d3c155cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_internlm2.py:   0%|          | 0.00/8.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d08d3b14554583b2715c5f8230060e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f00089cad24c5582a07792e873aa01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fecc5595ec4c878d31fccbc0a8d512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26552c0342343d78b574e1326a9359f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junyoon/.local/share/virtualenvs/RSP-HgGdQf5B/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junyoon/.local/share/virtualenvs/RSP-HgGdQf5B/lib/python3.10/site-packages/accelerate/utils/modeling.py:1674: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-11 22:26:38,994 - lmdeploy - \u001b[33mWARNING\u001b[0m - turbomind.py:217 - get 1541 model params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TM][WARNING] [LlamaTritonModel] `max_context_token_num` is not set, default to 8192.\n",
      "[TM][WARNING] pad vocab size from 92553 to 92554\n",
      "[TM][WARNING] pad embed size from 92554 to 92554\n",
      "[TM][WARNING] pad vocab size from 92553 to 92554\n",
      "[TM][WARNING] pad embed size from 92554 to 92554\n",
      "Convert to turbomind format:   0%|          | 0/48 [00:00<?, ?it/s]/home/junyoon/.local/share/virtualenvs/RSP-HgGdQf5B/lib/python3.10/site-packages/lmdeploy/turbomind/deploy/loader.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tmp = torch.load(shard, map_location='cpu')\n",
      "[TM][WARNING] Devicle 0 peer access Device 1 is not available.              \n",
      "[TM][WARNING] Devicle 1 peer access Device 0 is not available.\n",
      "[TM][WARNING] Devicle 1 peer access Device 2 is not available.\n",
      "[TM][WARNING] Devicle 0 peer access Device 2 is not available.\n",
      "[TM][WARNING] Devicle 1 peer access Device 3 is not available.\n",
      "[TM][WARNING] Devicle 0 peer access Device 3 is not available.\n",
      "[TM][WARNING] Devicle 1 peer access Device 4 is not available.\n",
      "[TM][WARNING] Devicle 1 peer access Device 5 is not available.\n",
      "[TM][WARNING] Devicle 0 peer access Device 4 is not available.\n",
      "[TM][WARNING] Devicle 1 peer access Device 6 is not available.\n",
      "[TM][WARNING] Devicle 0 peer access Device 5 is not available.\n",
      "[TM][WARNING] Devicle 1 peer access Device 7 is not available.\n",
      "[TM][WARNING] Devicle 0 peer access Device 6 is not available.\n",
      "[TM][WARNING] Devicle 0 peer access Device 7 is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n",
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n"
     ]
    }
   ],
   "source": [
    "from lmdeploy import pipeline, TurbomindEngineConfig\n",
    "from lmdeploy.vl import load_image\n",
    "path = 'OpenGVLab/InternVL2_5-26B'\n",
    "model = 'OpenGVLab/InternVL2_5-26B-AWQ'\n",
    "pipe = pipeline(model, backend_config=TurbomindEngineConfig(session_len=8192, tp=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbaba8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-11 22:28:29,581 - lmdeploy - \u001b[33mWARNING\u001b[0m - async_engine.py:629 - GenerationConfig: GenerationConfig(n=1, max_new_tokens=512, do_sample=False, top_p=1.0, top_k=50, min_p=0.0, temperature=0.8, repetition_penalty=1.0, ignore_eos=False, random_seed=None, stop_words=None, bad_words=None, stop_token_ids=[92542, 92540], bad_token_ids=None, min_new_tokens=None, skip_special_tokens=True, spaces_between_special_tokens=True, logprobs=None, response_format=None, logits_processors=None, output_logits=None, output_last_hidden_state=None)\n",
      "2025-02-11 22:28:29,583 - lmdeploy - \u001b[33mWARNING\u001b[0m - async_engine.py:630 - Since v0.6.0, lmdeploy add `do_sample` in GenerationConfig. It defaults to False, meaning greedy decoding. Please set `do_sample=True` if sampling  decoding is needed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two images show a robotic arm in a similar position, with a slight difference in the arm's orientation. The background is a plain, dark surface, and the robot is standing on a beige mat. In front of the robot, there is a wooden base with four vertical pegs of different colors: green, red, blue, and another red. The robot appears to be interacting with the pegs, possibly in a sorting or stacking task.\n"
     ]
    }
   ],
   "source": [
    "from lmdeploy.vl.constants import IMAGE_TOKEN\n",
    "image_path1 = \"/data/RSP/rlbench_frames/insert_onto_square_peg_variation0_episode0_front/pair_2_frame0.jpg\"  # Replace with your image path\n",
    "image_path2 = \"/data/RSP/rlbench_frames/insert_onto_square_peg_variation0_episode0_front/pair_2_frame1.jpg\"  # Replace with your image path\n",
    "\n",
    "response = pipe((f'Image-1: {IMAGE_TOKEN}\\nImage-2: {IMAGE_TOKEN}\\ndescribe these two images', [load_image(image_path1), load_image(image_path2)]))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27e7343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The robot arm has moved slightly to the right.\n",
      "- The arm's orientation has changed, now facing more towards the right.\n",
      "- The background remains static with no visible changes.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "response = pipe((f\"\"\"Image-1: {IMAGE_TOKEN}\\nImage-2: {IMAGE_TOKEN}\\nYou are a movement analyzer specialized in comparing consecutive video frames. Analyze and describe changes between frames using the following guidelines:\n",
    "FORMAT:\n",
    "- Provide one clear sentence summarizing the overall dynamics and activity\n",
    "- Describe qualitative and relative dynamics between frames\n",
    "- Use precise directional terms (left, right, up, down, forward, backward)\n",
    "- Focus on observable, concrete changes\n",
    "\n",
    "ANALYZE THE FOLLOWING ELEMENTS:\n",
    "- Main Subject/Object:\n",
    "    - Position: Track center of mass movement\n",
    "    - Rotation: Note any turns or spins\n",
    "    - Orientation: Describe facing direction\n",
    "    - State Changes: Document visible changes in:\n",
    "        - Physical form or shape\n",
    "        - Color or appearance\n",
    "        - Expression or emotional state (if applicable)\n",
    "- Background:\n",
    "    - Note any changes in background elements\n",
    "    - Identify moving vs static elements\n",
    "\n",
    "Keep descriptions concise, objective, and focused on visible changes between frames.\"\"\", [load_image(image_path1), load_image(image_path2)]))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c86b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSP-HgGdQf5B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
